{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUnSGHW78VYk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "1aSCYYnz8gFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/MyDrive/satellite\")"
      ],
      "metadata": {
        "id": "51_vDHjt8iaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_single = r\"/content/gdrive/MyDrive/satellite/TrainData/img/image_1610.h5\"\n",
        "path_single_mask = r\"/content/gdrive/MyDrive/satellite/TrainData/mask/mask_140.h5\""
      ],
      "metadata": {
        "id": "553hOasZ8nYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_data = np.zeros((1,128,128,3))\n",
        "with h5py.File(path_single) as hdf:\n",
        "  ls = list(hdf.keys())\n",
        "  print(\"ls\",ls)\n",
        "  data = np.array(hdf.get('img'))\n",
        "  print(\"input data shape: \", data.shape)\n",
        "  plt.imshow(data[:,:,3:0:-1])\n",
        "\n",
        "  data_red = data[:,:,3]\n",
        "  data_green = data[:, :, 2]\n",
        "  data_blue = data[:, :, 1]\n",
        "  data_nir = data[:, :, 7]\n",
        "  data_rgb = data[:, :, 3:0:-1]\n",
        "  data_ndvi = np.divide(data_nir - data_red,np.add(data_nir, data_red))\n",
        "  f_data[0, :, :, 0] =data_ndvi\n",
        "  f_data[0, :, :, 1] = data[:, :, 12]\n",
        "  f_data[0, :, :, 2] = data[:, :, 13]\n",
        "\n",
        "  print('data ndvi shape ', data_ndvi.shape,\"f_data shape: \",f_data.shape)\n",
        "  plt.imshow(data_ndvi)"
      ],
      "metadata": {
        "id": "KA6FF34P8pux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File(path_single_mask) as hdf:\n",
        "  ls = list(hdf.keys())\n",
        "\n",
        "  print(\"ls\",ls)\n",
        "  data = np.array(hdf.get('mask'))\n",
        "  print(\"input data shape: \",data.shape)\n",
        "  plt.imshow(data)"
      ],
      "metadata": {
        "id": "-5rBFTwt8r5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using all dataset"
      ],
      "metadata": {
        "id": "rh3GaLSg8v_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = r\"/content/gdrive/MyDrive/satellite/TrainData/img/*.h5\"\n",
        "TRAIN_MASK = r\"/content/gdrive/MyDrive/satellite/TrainData/mask/*.h5\"\n",
        "\n",
        "\n",
        "TRAIN_XX =  np.zeros((3799,128,128,6))\n",
        "TRAIN_YY =  np.zeros((3799,128,128,1))\n",
        "all_train = sorted(glob.glob(TRAIN_PATH))\n",
        "all_mask = sorted(glob.glob(TRAIN_MASK))"
      ],
      "metadata": {
        "id": "oDbbl-m38u8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train with RGB,NDVI,DEM and slope"
      ],
      "metadata": {
        "id": "umJeRcwp81YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testing for google colab GPU\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "rN0WAIOT80Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (img, mask) in enumerate(zip(all_train, all_mask)):\n",
        "    print(i, img, mask)\n",
        "    with h5py.File(img) as hdf:\n",
        "        ls = list(hdf.keys())\n",
        "        data = np.array(hdf.get('img'))\n",
        "\n",
        "        # assign 0 for the nan value\n",
        "        data[np.isnan(data)] = 0.000001\n",
        "\n",
        "        # to normalize the data\n",
        "        mid_rgb = data[:, :, 1:4].max() / 2.0\n",
        "        mid_slope = data[:, :, 12].max() / 2.0\n",
        "        mid_elevation = data[:, :, 13].max() / 2.0\n",
        "\n",
        "        # ndvi calculation\n",
        "        data_red = data[:, :, 3]\n",
        "        data_nir = data[:, :, 7]\n",
        "        data_ndvi = np.divide(data_nir - data_red,np.add(data_nir, data_red))\n",
        "\n",
        "        # final array\n",
        "        TRAIN_XX[i, :, :, 0] = 1 - data[:, :, 3] / mid_rgb  #RED\n",
        "        TRAIN_XX[i, :, :, 1] = 1 - data[:, :, 2] / mid_rgb #GREEN\n",
        "        TRAIN_XX[i, :, :, 2] = 1 - data[:, :, 1] / mid_rgb #BLUE\n",
        "        TRAIN_XX[i, :, :, 3] = data_ndvi #NDVI\n",
        "        TRAIN_XX[i, :, :, 4] = 1 - data[:, :, 12] / mid_slope #SLOPE\n",
        "        TRAIN_XX[i, :, :, 5] = 1 - data[:, :, 13] / mid_elevation #ELEVATION\n",
        "\n",
        "\n",
        "    with h5py.File(mask) as hdf:\n",
        "        ls = list(hdf.keys())\n",
        "        data=np.array(hdf.get('mask'))\n",
        "        TRAIN_YY[i, :, :, 0] = data"
      ],
      "metadata": {
        "id": "B-j_ljMm9hwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing min,max values in train data"
      ],
      "metadata": {
        "id": "U1Kx1sSd9BkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN_XX_N = TRAIN_XX / TRAIN_XX.max()\n",
        "TRAIN_XX[np.isnan(TRAIN_XX)] = 0.000001\n",
        "print(TRAIN_XX.min(),TRAIN_XX.max(),TRAIN_YY.min(),TRAIN_YY.max())"
      ],
      "metadata": {
        "id": "C14w_OZm9AIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom loss function(dice)"
      ],
      "metadata": {
        "id": "qrrZRtgN-Tss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_loss(Y_true,Y_pred):\n",
        "  Y_true = tf.cast(Y_true,tf.float32)\n",
        "  Y_pred = tf.math.sigmoid(Y_pred)\n",
        "  numerator = 2 * tf.reduce_sum(Y_true * Y_pred)\n",
        "  denominator = tf.reduce_sum(Y_true + Y_pred)\n",
        "  return 1 - numerator / denominator"
      ],
      "metadata": {
        "id": "LH58J9Ge-W0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visulization of the training data"
      ],
      "metadata": {
        "id": "kC5bqACC-b3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = 234\n",
        "fig,(ax1,ax2,ax3,ax4,ax5) = plt.subplots(1,5,figsize = (15,10))\n",
        "\n",
        "ax1.set_title(\"RGB image\")\n",
        "ax2.set_title(\"NDVI\")\n",
        "ax3.set_title(\"Slope\")\n",
        "ax4.set_title(\"Elevation\")\n",
        "ax5.set_title(\"Mask\")\n",
        "\n",
        "ax1.imshow(TRAIN_XX[img,:,:,0:3])\n",
        "ax2.imshow(TRAIN_XX[img,:,:,3])\n",
        "ax3.imshow(TRAIN_XX[img,:,:,4])\n",
        "ax4.imshow(TRAIN_XX[img,:,:,5])\n",
        "ax5.imshow(TRAIN_YY[img,:,:,0])\n"
      ],
      "metadata": {
        "id": "1IFST-Bo-a7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation split"
      ],
      "metadata": {
        "id": "hBBf-f6S-i76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#split the data\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(TRAIN_XX,TRAIN_YY,train_size = 0.2,shuffle = True)"
      ],
      "metadata": {
        "id": "sHAKVfwU-aLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = 234\n",
        "fig,(ax1,ax2,ax3,ax4,ax5) = plt.subplots(1,5,figsize=(18,6))\n",
        "\n",
        "rgb = X_train[img,:,:,0:3]\n",
        "rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
        "\n",
        "ax1.imshow(rgb)\n",
        "ax1.set_title(\"RGB image\")\n",
        "\n",
        "ax2.imshow(X_train[img,:,:,3], cmap='RdYlGn')\n",
        "ax2.set_title(\"NDVI\")\n",
        "\n",
        "ax3.imshow(X_train[img,:,:,4], cmap='terrain')\n",
        "ax3.set_title(\"Slope\")\n",
        "\n",
        "ax4.imshow(X_train[img,:,:,5], cmap='terrain')\n",
        "ax4.set_title(\"Elevation\")\n",
        "\n",
        "ax5.imshow(Y_train[img,:,:,0], cmap='gray')\n",
        "ax5.set_title(\"Mask\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CgiKT_q4-nhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape,Y_train.shape"
      ],
      "metadata": {
        "id": "SfhgyFe8-sCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to relese some memory, delete the un-necessary variables\n",
        "del TRAIN_XX\n",
        "del TRAIN_YY\n",
        "del all_train\n",
        "del all_mask"
      ],
      "metadata": {
        "id": "zz7VoNbwHPEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unet Model"
      ],
      "metadata": {
        "id": "WT11faphHQbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())       # get current working directory\n",
        "print(os.listdir())      # list all the files in directory\n"
      ],
      "metadata": {
        "id": "HhNSBS1-HPh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# recall\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "# precision\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "#f1 score\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "id": "KSYwyjifHUbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unet_model(IMG_WIDTH, IMG_HIGHT, IMG_CHANNELS):\n",
        "    inputs = tf.keras.layers.Input((IMG_WIDTH, IMG_HIGHT, IMG_CHANNELS))\n",
        "\n",
        "    # Converted inputs to floating\n",
        "    #s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "\n",
        "\n",
        "    #Contraction path\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "    c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
        "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "    c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
        "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "    c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
        "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "    #Expansive path\n",
        "    u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "    c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
        "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "\n",
        "    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
        "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
        "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "\n",
        "    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
        "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
        "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "\n",
        "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "FjfZvR0MHaKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = unet_model(128, 128, 6)\n",
        "# model.summary()\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(\"best_model.h5\", monitor=\"val_f1_m\", verbose=1, save_best_only=True, mode=\"max\")\n",
        "# earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_m', patience=10, verbose=1, mode='max')\n",
        "\n",
        "callbacks = [\n",
        "    # earlyStopping,\n",
        "    checkpointer\n",
        "    ]\n",
        "history = model.fit(X_train, Y_train, batch_size=16,\n",
        "          epochs=50,\n",
        "          verbose = 1,\n",
        "          validation_data=(X_valid, Y_valid),\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model.save(\"model_save.h5\")"
      ],
      "metadata": {
        "id": "dJVI23OuHd95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "vSPgp_uhIKjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig,((ax11,ax12),(ax13,ax14)) = plt.subplots(2,2,figsize=(20,15))\n",
        "ax11.plot(history.history['loss'])\n",
        "ax11.plot(history.history['val_loss'])\n",
        "ax11.title.set_text('Unet model loss')\n",
        "ax11.set_ylabel('loss')\n",
        "ax11.set_xlabel('epoch')\n",
        "ax11.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "ax12.plot(history.history['precision_m'])\n",
        "ax12.plot(history.history['val_precision_m'])\n",
        "ax12.set_title('Unet model precision')\n",
        "ax12.set_ylabel('precision')\n",
        "ax12.set_xlabel('epoch')\n",
        "ax12.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "ax13.plot(history.history['recall_m'])\n",
        "ax13.plot(history.history['val_recall_m'])\n",
        "ax13.set_title('Unet model recall')\n",
        "ax13.set_ylabel('recall')\n",
        "ax13.set_xlabel('epoch')\n",
        "ax13.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "ax14.plot(history.history['f1_m'])\n",
        "ax14.plot(history.history['val_f1_m'])\n",
        "ax14.set_title('Unet model f1')\n",
        "ax14.set_ylabel('f1')\n",
        "ax14.set_xlabel('epoch')\n",
        "ax14.legend(['train', 'validation'], loc='upper left')"
      ],
      "metadata": {
        "id": "KgXyeT4FHkFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "threshold = 0.5\n",
        "pred_img = model.predict(X_valid)\n",
        "pred_img = (pred_img > threshold).astype(np.uint8)"
      ],
      "metadata": {
        "id": "NF1HU__7I23r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "img = 339\n",
        "fig,(ax1,ax2,ax3)= plt.subplots(1,3,figsize=(15,10))\n",
        "ax1.imshow(pred_img[img, :, :, 0])\n",
        "ax1.set_title(\"Predictions\")\n",
        "ax2.imshow(Y_valid[img, :, :, 0])\n",
        "ax2.set_title(\"Label\")\n",
        "ax3.imshow(X_valid[img, :, :, 0:3])\n",
        "ax3.set_title('Training Image')"
      ],
      "metadata": {
        "id": "zZjoS1dgI3gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thank You!"
      ],
      "metadata": {
        "id": "bv1cH-tVMYaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hABUklZALy5h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}